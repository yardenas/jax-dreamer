defaults:
  # Train Script
  log_dir: results
  seed: 0
  task: Pendulum-v1
  time_limit: 1000
  action_repeat: 1
  steps: 1e6
  log_every: 1000
  training_steps_per_epoch: 2.5e4
  evaluation_steps_per_epoch: 1e4
  prefill: 500
  train_every: 1
  train_steps: 1
  replay: {capacity: 2e6, batch: 256}
  jit: True
  clip_rewards: none
  render_episodes: 1

  # World Model
  rssm: {hidden: 200, deter: 200, stoch: 32, discrete: 32, act: elu, min_std: 0.1}
  rssm_opt: {lr: 3e-4, eps: 1e-7, clip: 1e6}

  # Actor Critic
  actor: {layers: 2, units: 256, act: relu, min_std: 1e-5}
  critics: 2
  critic: {layers: 2, units: 256, act: relu}
  actor_opt: {lr: 3e-4, eps: 1e-7, clip: 1e6}
  critic_opt: {lr: 3e-4, eps: 1e-7, clip: 1e6}
  alpha_opt: {lr: 3e-4, eps: 1e-8, clip: 1e6}
  discount: 0.99
  imag_horizon: 15
  slow_target_update: 1
  tau: 5e-3
  reward_scale: 1.0

pendulum:
  training_steps_per_epoch: 1000
  evaluation_steps_per_epoch: 1000
  log_every: 200
  time_limit: 200
  prefill: 256
  steps: 1e4

mountaincar:
  task: MountainCarContinuous-v0
  training_steps_per_epoch: 15000
  evaluation_steps_per_epoch: 1000
  log_every: 500
  time_limit: 500
  prefill: 1500
  steps: 500000

lunarlander:
  task: LunarLanderContinuous-v2
  training_steps_per_epoch: 15000
  evaluation_steps_per_epoch: 1000
  log_every: 500
  time_limit: 500
  prefill: 1500
  steps: 500000


debug:
  jit: False
  time_limit: 200
  training_steps_per_epoch: 300
  evaluation_steps_per_epoch: 300
  prefill: 200
  render_episodes: 0
  replay: {capacity: 1000, batch: 32}